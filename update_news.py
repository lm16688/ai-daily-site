#!/usr/bin/env python3
"""
AI Daily æ–°é—»è‡ªåŠ¨æ›´æ–°è„šæœ¬
æ¯æ—¥æŠ“å–æœ€æ–° AI èµ„è®¯å¹¶ç”Ÿæˆ news_data.json

ä½¿ç”¨æ–¹æ³•:
1. å®‰è£…ä¾èµ–: pip install requests beautifulsoup4 feedparser --break-system-packages
2. è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆå¦‚ cronï¼‰æ¯å¤©è¿è¡Œä¸€æ¬¡
3. è„šæœ¬ä¼šè‡ªåŠ¨æŠ“å–ã€åˆ†ç±»ã€å»é‡ï¼Œå¹¶è¾“å‡º JSON æ–‡ä»¶
"""

import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import feedparser
import re

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®åŒº
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# RSS æ–°é—»æºï¼ˆçœŸå®å¯é çš„ AI æ–°é—»æºï¼‰
RSS_FEEDS = [
    'https://techcrunch.com/tag/artificial-intelligence/feed/',
    'https://www.technologyreview.com/topic/artificial-intelligence/feed',
    'https://venturebeat.com/category/ai/feed/',
    'https://www.artificialintelligence-news.com/feed/',
]

# åˆ†ç±»å…³é”®è¯æ˜ å°„
CATEGORY_KEYWORDS = {
    'news': ['breakthrough', 'announce', 'launch', 'release', 'trend', 'industry', 'market', 'partnership', 'acquisition'],
    'tools': ['tool', 'platform', 'app', 'software', 'service', 'product', 'chatgpt', 'claude', 'gemini', 'copilot'],
    'research': ['research', 'paper', 'study', 'model', 'algorithm', 'deepmind', 'openai', 'anthropic', 'breakthrough'],
    'industry': ['business', 'investment', 'funding', 'revenue', 'market', 'enterprise', 'startup', 'ipo'],
    'safety': ['safety', 'ethics', 'regulation', 'policy', 'privacy', 'bias', 'risk', 'governance', 'law'],
}

# çƒ­ç‚¹å…³é”®è¯ï¼ˆåŒ…å«è¿™äº›è¯çš„ä¼šæ ‡è®°ä¸º hotï¼‰
HOT_KEYWORDS = ['breakthrough', 'chatgpt', 'openai', 'google', 'microsoft', 'anthropic', 'billion', 'major', 'revolutionary']

OUTPUT_FILE = 'news_data.json'
MAX_ARTICLES = 30  # æ¯æ¬¡æœ€å¤šä¿ç•™çš„æ–‡ç« æ•°

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å·¥å…·å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºæ ¼ã€HTML æ ‡ç­¾ç­‰"""
    if not text:
        return ""
    # ç§»é™¤ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def categorize_article(title, summary):
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨åˆ†ç±»æ–‡ç« """
    text = (title + ' ' + summary).lower()
    
    scores = {}
    for cat, keywords in CATEGORY_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in text)
        scores[cat] = score
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†ç±»ï¼Œé»˜è®¤ news
    if max(scores.values()) == 0:
        return 'news'
    return max(scores, key=scores.get)

def is_hot(title, summary):
    """åˆ¤æ–­æ˜¯å¦ä¸ºçƒ­ç‚¹æ–°é—»"""
    text = (title + ' ' + summary).lower()
    return any(kw in text for kw in HOT_KEYWORDS)

def extract_tags(title, summary):
    """ä»æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–æ ‡ç­¾"""
    text = title + ' ' + summary
    tags = []
    
    # å¸¸è§ AI å·¥å…·/å…¬å¸å
    entities = ['ChatGPT', 'OpenAI', 'Google', 'Microsoft', 'Anthropic', 'Claude', 
                'DeepMind', 'Meta', 'Apple', 'Amazon', 'Tesla', 'Nvidia']
    for entity in entities:
        if entity.lower() in text.lower():
            tags.append(entity)
    
    # æŠ€æœ¯å…³é”®è¯
    tech_keywords = ['æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'LLM', 'æ™ºèƒ½ä½“', 'AGI', 
                     'ç”Ÿæˆå¼AI', 'è®¡ç®—æœºè§†è§‰', 'NLP', 'å¼ºåŒ–å­¦ä¹ ']
    for kw in tech_keywords:
        if kw in text:
            tags.append(kw)
    
    return tags[:5]  # æœ€å¤šè¿”å› 5 ä¸ªæ ‡ç­¾

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ¸å¿ƒæŠ“å–å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_from_rss(feed_url):
    """ä» RSS æºæŠ“å–æ–‡ç« """
    articles = []
    
    try:
        print(f"  æ­£åœ¨æŠ“å–: {feed_url}")
        feed = feedparser.parse(feed_url)
        
        for entry in feed.entries[:10]:  # æ¯ä¸ªæºå–å‰ 10 æ¡
            title = clean_text(entry.get('title', ''))
            summary = clean_text(entry.get('summary', entry.get('description', '')))
            
            # é™åˆ¶æ‘˜è¦é•¿åº¦
            if len(summary) > 200:
                summary = summary[:197] + '...'
            
            # æå–å‘å¸ƒæ—¥æœŸ
            pub_date = entry.get('published_parsed') or entry.get('updated_parsed')
            if pub_date:
                date_str = time.strftime('%Y-%m-%d', pub_date)
            else:
                date_str = datetime.now().strftime('%Y-%m-%d')
            
            # æå–æ¥æº
            source = feed.feed.get('title', 'Unknown')
            
            # æ–‡ç« é“¾æ¥
            url = entry.get('link', '#')
            
            if title and summary:
                article = {
                    'title': title,
                    'summary': summary,
                    'source': source,
                    'date': date_str,
                    'url': url,
                    'cat': categorize_article(title, summary),
                    'hot': is_hot(title, summary),
                    'tags': extract_tags(title, summary) or ['AI'],
                }
                articles.append(article)
        
        print(f"    âœ“ æˆåŠŸæŠ“å– {len(articles)} ç¯‡")
        
    except Exception as e:
        print(f"    âœ— æŠ“å–å¤±è´¥: {e}")
    
    return articles

def deduplicate_articles(articles):
    """å»é‡ï¼šåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦"""
    seen_titles = set()
    unique_articles = []
    
    for article in articles:
        # ç®€å•çš„å»é‡é€»è¾‘ï¼šæ ‡é¢˜çš„å‰ 30 ä¸ªå­—ç¬¦
        title_key = article['title'][:30].lower()
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_articles.append(article)
    
    return unique_articles

def prioritize_articles(articles):
    """ä¼˜å…ˆçº§æ’åºï¼šçƒ­ç‚¹ä¼˜å…ˆï¼Œç„¶åæŒ‰æ—¥æœŸ"""
    return sorted(articles, key=lambda x: (not x['hot'], x['date']), reverse=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("=" * 60)
    print("AI Daily æ–°é—»è‡ªåŠ¨æ›´æ–°è„šæœ¬")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    all_articles = []
    
    # 1. ä»æ‰€æœ‰ RSS æºæŠ“å–
    print("ğŸ“¡ ç¬¬ä¸€æ­¥ï¼šæŠ“å–æ–°é—»æº")
    for feed_url in RSS_FEEDS:
        articles = fetch_from_rss(feed_url)
        all_articles.extend(articles)
        time.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ
    
    print(f"\n  å…±æŠ“å– {len(all_articles)} ç¯‡åŸå§‹æ–‡ç« ")
    
    # 2. å»é‡
    print("\nğŸ” ç¬¬äºŒæ­¥ï¼šå»é‡å¤„ç†")
    all_articles = deduplicate_articles(all_articles)
    print(f"  å»é‡åå‰©ä½™ {len(all_articles)} ç¯‡")
    
    # 3. æ’åºå’Œæˆªæ–­
    print("\nâ­ ç¬¬ä¸‰æ­¥ï¼šä¼˜å…ˆçº§æ’åº")
    all_articles = prioritize_articles(all_articles)
    all_articles = all_articles[:MAX_ARTICLES]
    print(f"  ä¿ç•™å‰ {len(all_articles)} ç¯‡")
    
    # 4. æ·»åŠ  ID
    for i, article in enumerate(all_articles, 1):
        article['id'] = i
    
    # 5. ç”Ÿæˆ JSON
    print("\nğŸ’¾ ç¬¬å››æ­¥ï¼šä¿å­˜ JSON æ–‡ä»¶")
    output_data = {
        'last_update': datetime.now().isoformat(),
        'total_count': len(all_articles),
        'articles': all_articles
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ“ å·²ä¿å­˜è‡³ {OUTPUT_FILE}")
    
    # 6. ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ–‡ç« æ•°: {len(all_articles)}")
    print(f"  çƒ­ç‚¹æ–‡ç« : {sum(1 for a in all_articles if a['hot'])}")
    
    cat_counts = {}
    for article in all_articles:
        cat = article['cat']
        cat_counts[cat] = cat_counts.get(cat, 0) + 1
    
    for cat, count in sorted(cat_counts.items()):
        print(f"  {cat}: {count}")
    
    print("\n" + "=" * 60)
    print(f"âœ… æ›´æ–°å®Œæˆï¼æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

if __name__ == '__main__':
    main()
